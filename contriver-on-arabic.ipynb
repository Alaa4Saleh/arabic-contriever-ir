{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install transformers faiss-cpu ranx tqdm ir_datasets\n\nimport os, math, time, json\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport faiss\nimport ir_datasets\nfrom ranx import Qrels, Run, evaluate\n\n# ---------- Config ----------\nWORKDIR = Path(\"/kaggle/working\")\nEMB_DIR = WORKDIR / \"miracl_ar_contriever_emb_chunks\"\nRUN_DIR = WORKDIR / \"runs\"\nEMB_DIR.mkdir(parents=True, exist_ok=True)\nRUN_DIR.mkdir(parents=True, exist_ok=True)\n\nMODEL_NAME = \"facebook/contriever\"\nDATASET = \"miracl/ar/dev\"     # queries + qrels\nCORPUS  = \"miracl/ar\"         # corpus\n\nCHUNK_SIZE = 50_000\nBATCH_SIZE = 128\nMAX_LENGTH = 256\n\nTOPK = 1000\n\n# IVF params (good for 2M docs)\nN_LIST = 8192\nN_PROBE = 16\nTRAIN_VECS = 400_000  # better than 200k if possible\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------- Model ----------\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\ndef mean_pooling(last_hidden_state, attention_mask):\n    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n    summed = (last_hidden_state * mask).sum(dim=1)\n    counts = mask.sum(dim=1).clamp(min=1e-9)\n    return summed / counts\n\n@torch.no_grad()\ndef encode_texts(texts, batch_size=BATCH_SIZE):\n    embs = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        tok = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n        tok = {k: v.to(device) for k, v in tok.items()}\n        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n            out = model(**tok)\n            emb = mean_pooling(out.last_hidden_state, tok[\"attention_mask\"])\n            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n        embs.append(emb.detach().cpu().numpy().astype(\"float32\"))\n    return np.vstack(embs)\n\ndef chunk_paths(chunk_idx: int):\n    emb_path = EMB_DIR / f\"emb_{chunk_idx:05d}.npy\"\n    ids_path = EMB_DIR / f\"ids_{chunk_idx:05d}.npy\"\n    meta_path = EMB_DIR / f\"meta_{chunk_idx:05d}.json\"\n    return emb_path, ids_path, meta_path\n\n# ---------- Load datasets ----------\nds = ir_datasets.load(DATASET)     # has queries + qrels + (docs_iter too)\ncorpus_ds = ir_datasets.load(CORPUS)\n\nnum_docs = corpus_ds.docs_count()\nnum_chunks = math.ceil(num_docs / CHUNK_SIZE)\nprint(\"Corpus docs:\", num_docs, \"Num chunks:\", num_chunks)\n\n# ---------- 1) Encode corpus in chunks (SKIP if exists) ----------\ndocs_iter = corpus_ds.docs_iter()\nchunk_idx = 0\nbuf_ids, buf_texts = [], []\nseen = 0\n\nfor d in tqdm(docs_iter, total=num_docs, desc=\"Streaming corpus\"):\n    doc_id = str(d.doc_id)\n    title = (d.title or \"\").strip()\n    text = (d.text or \"\").strip()\n    full = (title + \"\\n\" + text).strip() if title else text\n\n    buf_ids.append(doc_id)\n    buf_texts.append(full)\n    seen += 1\n\n    if len(buf_ids) == CHUNK_SIZE or seen == num_docs:\n        emb_path, ids_path, meta_path = chunk_paths(chunk_idx)\n\n        if emb_path.exists() and ids_path.exists():\n            # skip\n            pass\n        else:\n            t0 = time.time()\n            emb = encode_texts(buf_texts)\n            dt = time.time() - t0\n            np.save(emb_path, emb)\n            np.save(ids_path, np.array(buf_ids, dtype=object))\n            meta_path.write_text(json.dumps({\n                \"chunk_idx\": chunk_idx, \"n\": len(buf_ids), \"seconds\": dt\n            }, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n            print(f\"✅ saved chunk {chunk_idx} n={len(buf_ids)} time={dt/60:.1f}m\")\n\n        buf_ids, buf_texts = [], []\n        chunk_idx += 1\n\nprint(\"✅ All chunks ready.\")\n\n# ---------- 2) Build FAISS IVF index ----------\nfirst = np.load(chunk_paths(0)[0])\ndim = first.shape[1]\nprint(\"Embedding dim:\", dim)\n\n# collect training vectors\nrng = np.random.default_rng(42)\ntrain_buf = []\nneed = TRAIN_VECS\n\nfor ci in range(num_chunks):\n    emb = np.load(chunk_paths(ci)[0]).astype(\"float32\")\n    if emb.shape[0] <= need:\n        train_buf.append(emb)\n        need -= emb.shape[0]\n    else:\n        idx = rng.choice(emb.shape[0], size=need, replace=False)\n        train_buf.append(emb[idx])\n        need = 0\n    if need == 0:\n        break\n\ntrain_x = np.vstack(train_buf)\nprint(\"Train sample:\", train_x.shape)\n\nquantizer = faiss.IndexFlatIP(dim)\nindex = faiss.IndexIVFFlat(quantizer, dim, N_LIST, faiss.METRIC_INNER_PRODUCT)\nprint(\"Training IVF...\")\nindex.train(train_x)\nindex.nprobe = N_PROBE\nprint(\"✅ IVF trained. Adding vectors...\")\n\nall_doc_ids = []\ntotal = 0\nfor ci in range(num_chunks):\n    emb = np.load(chunk_paths(ci)[0]).astype(\"float32\")\n    ids = np.load(chunk_paths(ci)[1], allow_pickle=True)\n    index.add(emb)\n    all_doc_ids.append(ids)\n    total += emb.shape[0]\n    if ci % 10 == 0:\n        print(f\"  added {ci}/{num_chunks-1}, total={total}\")\n\nall_doc_ids = np.concatenate(all_doc_ids)\nprint(\"Index total:\", index.ntotal, \"Docids:\", len(all_doc_ids))\n\nINDEX_PATH = WORKDIR / \"faiss_miracl_ar_contriever_ivf.index\"\nDOCIDS_PATH = WORKDIR / \"faiss_miracl_ar_docids.npy\"\nfaiss.write_index(index, str(INDEX_PATH))\nnp.save(DOCIDS_PATH, all_doc_ids)\nprint(\"✅ Saved index + docids\")\n\n# ---------- 3) Encode queries ----------\nqueries = list(ds.queries_iter())\nquery_ids = [str(q.query_id) for q in queries]\nquery_texts = [q.text for q in queries]\nq_emb = encode_texts(query_texts, batch_size=64)\nprint(\"Queries:\", len(query_ids), \"q_emb:\", q_emb.shape)\n\n# ---------- 4) Search + write run ----------\nscores, idxs = index.search(q_emb.astype(\"float32\"), TOPK)\nrun_path = RUN_DIR / \"contriever_zero_miracl_ar_dev.run\"\nwith open(run_path, \"w\", encoding=\"utf-8\") as f:\n    for i, qid in enumerate(query_ids):\n        for rank, (doc_idx, score) in enumerate(zip(idxs[i], scores[i]), start=1):\n            if doc_idx < 0:\n                continue\n            docid = str(all_doc_ids[int(doc_idx)])\n            f.write(f\"{qid} Q0 {docid} {rank} {float(score)} contriever_zero\\n\")\nprint(\"✅ Wrote run:\", run_path)\n\n# ---------- 5) Eval (qrels from SAME ds) ----------\nqrels_dict = {}\nfor r in ds.qrels_iter():\n    qrels_dict.setdefault(str(r.query_id), {})\n    qrels_dict[str(r.query_id)][str(r.doc_id)] = int(r.relevance)\n\nqrels = Qrels(qrels_dict)\nrun = Run.from_file(str(run_path), kind=\"trec\")\n\nmetrics = [\"map\", \"mrr\", \"ndcg@10\", \"recall@100\"]\nresults = evaluate(qrels, run, metrics)\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:03:47.138644Z","iopub.execute_input":"2026-02-14T15:03:47.139280Z","iopub.status.idle":"2026-02-14T17:12:45.985284Z","shell.execute_reply.started":"2026-02-14T15:03:47.139250Z","shell.execute_reply":"2026-02-14T17:12:45.984371Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nCorpus docs: 2061414 Num chunks: 42\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:   2%|▏         | 45544/2061414 [00:00<00:30, 65106.60it/s]/tmp/ipykernel_55/4293415551.py:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\nStreaming corpus:   3%|▎         | 64452/2061414 [02:56<2:14:19, 247.77it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 0 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:   6%|▌         | 114911/2061414 [05:51<2:05:15, 259.01it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 1 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:   8%|▊         | 164167/2061414 [08:46<2:07:19, 248.34it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 2 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  10%|█         | 215430/2061414 [11:39<1:57:22, 262.13it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 3 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  13%|█▎        | 266034/2061414 [14:32<1:56:01, 257.92it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 4 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  15%|█▌        | 315632/2061414 [17:23<1:43:55, 279.99it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 5 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  18%|█▊        | 366154/2061414 [20:15<1:50:24, 255.90it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 6 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  20%|██        | 417730/2061414 [23:07<1:31:50, 298.28it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 7 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  22%|██▏       | 457903/2061414 [25:55<2:16:26, 195.88it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 8 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  25%|██▌       | 515563/2061414 [28:48<1:31:37, 281.17it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 9 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  27%|██▋       | 565927/2061414 [31:40<1:37:48, 254.84it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 10 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  30%|██▉       | 616167/2061414 [34:32<1:34:20, 255.33it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 11 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  32%|███▏      | 657855/2061414 [37:24<1:58:45, 196.97it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 12 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  35%|███▍      | 715857/2061414 [40:17<1:28:27, 253.54it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 13 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  37%|███▋      | 757847/2061414 [43:09<2:13:16, 163.02it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 14 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  40%|███▉      | 816804/2061414 [46:01<1:11:18, 290.92it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 15 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  42%|████▏     | 867077/2061414 [48:53<1:07:31, 294.82it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 16 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  44%|████▍     | 907499/2061414 [51:44<1:39:36, 193.07it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 17 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  47%|████▋     | 967604/2061414 [54:37<1:06:04, 275.90it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 18 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  49%|████▉     | 1006270/2061414 [57:28<1:38:01, 179.40it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 19 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  51%|█████▏    | 1057718/2061414 [1:00:21<1:34:00, 177.96it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 20 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  54%|█████▍    | 1117706/2061414 [1:03:14<57:14, 274.78it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 21 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  57%|█████▋    | 1167250/2061414 [1:06:07<51:31, 289.21it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 22 n=50000 time=2.9m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  59%|█████▊    | 1207997/2061414 [1:08:54<1:13:06, 194.56it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 23 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  61%|██████    | 1257891/2061414 [1:11:19<55:52, 239.67it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 24 n=50000 time=2.4m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  64%|██████▍   | 1316484/2061414 [1:14:09<42:57, 289.02it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 25 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  66%|██████▌   | 1359200/2061414 [1:16:54<54:26, 214.95it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 26 n=50000 time=2.7m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  69%|██████▊   | 1415449/2061414 [1:19:44<38:59, 276.14it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 27 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  71%|███████   | 1466119/2061414 [1:22:34<34:21, 288.81it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 28 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  74%|███████▎  | 1517902/2061414 [1:25:26<30:09, 300.35it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 29 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  76%|███████▌  | 1559477/2061414 [1:28:15<41:16, 202.65it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 30 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  79%|███████▊  | 1618372/2061414 [1:31:04<25:24, 290.66it/s]  ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 31 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  81%|████████  | 1671305/2061414 [1:33:44<18:00, 361.08it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 32 n=50000 time=2.7m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  83%|████████▎ | 1707814/2061414 [1:36:29<26:51, 219.48it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 33 n=50000 time=2.7m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  85%|████████▌ | 1757627/2061414 [1:39:19<26:12, 193.23it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 34 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  88%|████████▊ | 1817374/2061414 [1:42:09<13:30, 301.11it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 35 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  91%|█████████ | 1867880/2061414 [1:45:00<10:50, 297.39it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 36 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  93%|█████████▎| 1909904/2061414 [1:47:51<12:12, 206.96it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 37 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  96%|█████████▌| 1968773/2061414 [1:50:38<05:04, 304.00it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 38 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus:  97%|█████████▋| 2008630/2061414 [1:53:24<04:35, 191.61it/s] ","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 39 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus: 100%|█████████▉| 2059790/2061414 [1:56:14<00:08, 196.22it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 40 n=50000 time=2.8m\n","output_type":"stream"},{"name":"stderr","text":"Streaming corpus: 100%|██████████| 2061414/2061414 [1:56:52<00:00, 293.97it/s]","output_type":"stream"},{"name":"stdout","text":"✅ saved chunk 41 n=11414 time=0.6m\n✅ All chunks ready.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Embedding dim: 768\nTrain sample: (400000, 768)\nTraining IVF...\n✅ IVF trained. Adding vectors...\n  added 0/41, total=50000\n  added 10/41, total=550000\n  added 20/41, total=1050000\n  added 30/41, total=1550000\n  added 40/41, total=2050000\nIndex total: 2061414 Docids: 2061414\n✅ Saved index + docids\nQueries: 2896 q_emb: (2896, 768)\n✅ Wrote run: /kaggle/working/runs/contriever_zero_miracl_ar_dev.run\n{'map': np.float64(0.0006234993661948514), 'mrr': np.float64(0.0006794800701671288), 'ndcg@10': np.float64(0.0007499007726032625), 'recall@100': np.float64(0.0024171270718232043)}\n","output_type":"stream"}],"execution_count":19}]}